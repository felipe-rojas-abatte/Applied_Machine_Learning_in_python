{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Lineales\n",
    "Un modelo lineal es una suma de variables pesadas que predice el valor de salida dado un valor de entrada.\n",
    "\n",
    "Ej: Predecir el valor del precio de casas.\n",
    "Dadas algunas características de las casas como: impuestos por año ($X_{tax}$) y edad de la casa en años ($X_{age}$) podemos utilizar esa información para estimar el precio de cada casa. Como ejemplo podemos escribir el siguiente modelo.\n",
    "\n",
    "$Y_{price} = C_{value} + w_{tax}X_{tax} + w_{age}X_{tax}$\n",
    "\n",
    "La idea principal es que usando ML podemos ajustar los valores de los pesos $w_i$ y el valor constante para poder predecir valores de casas basados en inputs anteriores.\n",
    "\n",
    "Este es un claro ejemplo de un modelo de regresion lineal.\n",
    "\n",
    "### Estructura de un modelo de regresion lineal\n",
    "De forma general este tipo de modelos tiene como input un conjunto de cracterísticas denominadas $x_i$, $i=0,\\dots,n$. Y cada característica se le asocia un peso $w_i$, $i=0,\\dots,n$, de modo que el valor de salida sará la suma de los pesos por cada caracteristica mas un valor constante $b$:\n",
    "\n",
    "$\\hat{y} = \\sum_i \\hat{w}_ix_i + \\hat{b}$\n",
    "\n",
    "Los pesos $\\hat{w}_i$ y el intercepto $\\hat{b}$ son los parametros a estimar basados en el entrenamiendo del modelo. Una forma de encontrar estos parametros es a través del método de los mínimos cuadrados.\n",
    "Este método consiste en encontrar el valor que minimice el error medio cuadrado del modelo, es decir, el valor que minimice la suma de las diferencias al cuadrado entre el valor predicho de cada punto $\\hat{y}(x_i)$ y el valor real de cada punto $y(x)$, dividido por el número de puntos utilizados en el entrenamiento.\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=0}^n (\\hat{y}(x_i) - y(x))^2$\n",
    "\n",
    "Los modelos lineales hacen fuertes suposiciones sobre la relación de la variables de entrada y de salida. Pareciera que fueran simplistas, pero en datasets con muchas variables pueden ser muy efectivos y generalizar bien la dinámica de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Como son estimados los parámetros del modelo?\n",
    "Existen varias formas de estimar los parámetros del modelo a partir de los datos de entrenamiento. Esto va a depender de los criterios de fiteo, objetivos y formas de controlar la complejidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como implementar la Regresion Lineal\n",
    "Al igual que en los casos anteriores usamos la clase LinearRegression con los datos ya separados que usaremos para entrenar el modelo.\n",
    "Una vez entrenado, podemos acceder a los valores de $w_i$ y $b$ con el atributo coef_ y intercept_ respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model coeff (w): [45.70870465]\n",
      "linear model intercept (b): 148.446\n",
      "R-squared score (training): 0.679\n",
      "R-squared score (test): 0.492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
    "m = linreg.coef_\n",
    "b = linreg.intercept_\n",
    "y = m*X_R1 + b\n",
    "plt.plot(X_R1, y, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación del K-NN Regression vs Least-Squares Linear Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante comparar los resultados de una regresión lineal aplicando dos modelos distintos de ML al mismo conjunto de datos:\n",
    "- k-NN Regression\n",
    "- Least-Squares Lineal Regression\n",
    "\n",
    "En primer lugar el modelo k-NN no hace muchas suposiciones sobre la estructurta de los datos, por lo que su predicción es potencialmente  precisa pero algunas veces genera predicciones inestable que son muy sensibles a pequeños cambios en los datos de entrenamiento. Esto hace que presente coeficiente de determinación mayor (para el training set) que el caso de regresión lineal.\n",
    "\n",
    "Por el contrario, modelos lineales asumen muchas suposiciones sobre la estructura de los datos, dando como resultados predicciones mas estables pero a la vez más imprecisas, mostrando un coeficiente de determinación menor (para el training set) que el caso k-NN.\n",
    "\n",
    "<img src=\"k-NN_vs_LS_linear.png\">\n",
    "\n",
    "Para este caso particular, es importante resaltar que el modelo de regresion lineal obtuvo un $R^2$ ligeramente más alto que el caso k-NN para los datos de test, debido a que existía una clara relación entre las variables haciendo que estos se ajustaran de mejor forma a la predicción. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras formas de estimar los parámatros del modelo lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Usa el mismo criterio de mínimos cuadrados que el caso anterior, con una pequeña diferencia, agrega una penalización a los parametros $w_i$ que son muy grandes (a esto se le llama regularización). La reguralización es importante en ML dado que previene un sobreajuste (overfit) del modelo, reduciendo la complejidad de este.  \n",
    "\n",
    "$RSS_{RIDGE}(w,b) = \\sum_{i=1}^N (y(x_i) - (w\\cdot x_i + b))^2 + \\alpha \\sum_{j=1}^p w_j^2$\n",
    "\n",
    "La incorporación de la última parte en la ecuación mostrada hace que modelos con pesos más grandes contribuyan más en la ecuación final. De esta forma, debido a que el objetivo final es minimizar la funcion final, la regularización actua como una penalización ($L_2$) a los pesos que mas contribuyen. Así, en el caso en que tengamos 2 posibles predicciones, se preferirá un modelo donde la suma de los pesos total sea menor. Este efecto no se aprecia en casos con 1 sola característica, sino que en datasets con múltiples caracteristicas, donde la regularización puede incrementar de forma importante la eficiencia del modelo.\n",
    "\n",
    "La cantidad de regularización que aplicamos está controlada por el parámetro $\\alpha$, indicando que valores más altos de $\\alpha$ implican mayor regularización y por lo tanto menor complejidad en los modelo con pesos más cercanos a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación de modelo de Regresión Ridge\n",
    "Primero cargamos el dataframe con los datos de crimenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import load_crime_dataset\n",
    "crime = pd.read_table('/home/felipe/Documents/Programas/git_repository/Applied_Machine_Learning_in_python/2nd_week/CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "# remove features with poor coverage or lower relevance, and keep ViolentCrimesPerPop target column\n",
    "columns_to_keep = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]  \n",
    "crime = crime.ix[:,columns_to_keep].dropna()\n",
    "X_crime = crime.ix[:,range(0,88)]\n",
    "y_crime = crime['ViolentCrimesPerPop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la clase Ridge y pasando la información del parametro $\\alpha$ podemos inmediatamente entrenar el modelo con la cunción fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "ridge intercept: -3352.42, linear intercept: 3861.71\n",
      "R-squared score (training): ridge: 0.671,  linear: 0.668\n",
      "R-squared score (test): ridge: 0.494,  linear: 0.520\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge intercept: {:.2f}, linear intercept: {:.2f}'\n",
    "     .format(linridge.intercept_,linreg.intercept_))\n",
    "print('R-squared score (training): ridge: {:.3f},  linear: {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train),linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): ridge: {:.3f},  linear: {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test),linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos el resultado de Ridge con el modelo de regresion lineal simple podemos ver que no hay mayor diferencia en los valores de $R^2$. Sin embargo, podemos realizar algunos cambios a modelo Ridge para mejorar los resultados. Lo que podemos hacer es normalizar las escalas de los pesos de cada característica que originalmente presentaban distintas escalas. Existen varias formas de normalizar los pesos, pero en esta oportunidad utilizaremos un tipo llamado reescalamiento MinMax, haciendo que todas las características estén evaluadas entre 0 y 1:\n",
    "- A cada caracteristica $x_i$ calcularemos el valor mínimo y máximo y transformaremos de $x_i$ a $x'_i$ de la siguiente forma:\n",
    "\n",
    "$x'_i = \\frac{x_i - x_i^{min}}{x_i^{max} - x_i^{min}}$\n",
    "\n",
    "Para esto debemos:\n",
    "- importar MinMaxScaler \n",
    "- preparar el objeto escalar para su uso usando scaler.fit(X_train)\n",
    "- aplicamos el reescalamiento con scaler.transform()\n",
    "- Podemos ser un poco mas eficientes aplicando al mismo el fit y la transformación al training set con el comando scaler.fit_transform(X_train)\n",
    "\n",
    "Algunos consejos a la hora de realizar la normalizacon:\n",
    "- fitear el scaler usando el trainig set y luego aplicar el mnismo scaler al train set.\n",
    "- No escalar el training y test set usando diferentes scalers: esto causaría un sesgo en los datos.\n",
    "- No fit el scaler con el test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "ridge scaled intercept: 933.39, linear intercept: 3861.71\n",
      "R-squared score (training): ridge scaled: 0.615,  linear: 0.668\n",
      "R-squared score (test): ridge scaled: 0.599,  linear: 0.520\n",
      "Improvement of 21.16% in the test set prediction\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "#Aplicamos el reescalamiento a X_train y X_test\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge_scaled = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge scaled intercept: {:.2f}, linear intercept: {:.2f}'\n",
    "     .format(linridge_scaled.intercept_,linreg.intercept_))\n",
    "print('R-squared score (training): ridge scaled: {:.3f},  linear: {:.3f}'\n",
    "     .format(linridge_scaled.score(X_train_scaled, y_train),linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): ridge scaled: {:.3f},  linear: {:.3f}'\n",
    "     .format(linridge_scaled.score(X_test_scaled, y_test),linreg.score(X_test, y_test)))\n",
    "eff = 100*(linridge_scaled.score(X_test_scaled, y_test)- linridge.score(X_test, y_test))/linridge.score(X_test, y_test)\n",
    "print('Improvement of {:.2f}% in the test set prediction'.format(eff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Que ocurre si variamos $\\alpha$ en el modelo Ridge?\n",
    "El mejor $R^2$ en el test set resulta ser cuando usamos $\\alpha=20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: effect of alpha regularization parameter\n",
      "\n",
      "Alpha = 0.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.67, r-squared test: 0.50\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.66, r-squared test: 0.56\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.63, r-squared test: 0.59\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 88, r-squared training: 0.61, r-squared test: 0.60\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 86, r-squared training: 0.58, r-squared test: 0.58\n",
      "\n",
      "Alpha = 100.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.55, r-squared test: 0.55\n",
      "\n",
      "Alpha = 1000.00\n",
      "num abs(coeff) > 1.0: 84, r-squared training: 0.31, r-squared test: 0.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Al igual que el metodo de Ridge, Lasso también agrega un factor de penalización ($L_1$) a los pesos, pero esta vez usa el valor absoluto del peso en vez del peso al cuadrado\n",
    "\n",
    "$RSS_{LASSO}(w,b) = \\sum_{i=1}^N (y(x_i) - (w\\cdot x_i + b))^2 + \\alpha \\sum_{j=1}^p |w_j|$\n",
    "\n",
    "El efecto de esta forma de regularización es hacer que los pesos de los parametros que menos contribuyen sean 0, dejando con un valor distinto de 0 a los pesos más influyentes. Esta forma de regularización también esta controlada por el parámetro $\\alpha$.\n",
    "\n",
    "Basados en la forma en como se regularizan las regresiones lineales, ya sea usando Ridge ($L_2$) o Lasso ($L_1$), podemos distinguir en que casos usar una o la otra forma:\n",
    "- Cuando tenemos muchas características ($x_i$) con efectos pequeños o medianos usamos Ridge\n",
    "- Cuando tenemos pocas caractetísticas ($x_i$) con efectos medianos o largos usamos Lasso\n",
    "\n",
    "Para implementar la regresión Lasso debemos importar la clase Lasso.\n",
    "En algunos casos, aparecerá en pantalla una advertencia de convergencia. Para evitar eso debemos incorporar la variable max_iter con un valor grande, frecuentemente se usa por lo menos 20000 o más. Esto aumentará el tiempo de iteración.\n",
    "Usando el dataset de los crimenes podemos ver que solo algunos pesos han quedado con valores distintos de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "lasso regression linear model intercept: 1186.6120619985793\n",
      "lasso regression linear model coeff:\n",
      "[    0.             0.            -0.          -168.18346054\n",
      "    -0.            -0.             0.           119.6938194\n",
      "     0.            -0.             0.          -169.67564456\n",
      "    -0.             0.            -0.             0.\n",
      "     0.             0.            -0.            -0.\n",
      "     0.            -0.             0.             0.\n",
      "   -57.52991966    -0.            -0.             0.\n",
      "   259.32889226    -0.             0.             0.\n",
      "     0.            -0.         -1188.7396867     -0.\n",
      "    -0.            -0.          -231.42347299     0.\n",
      "  1488.36512229     0.            -0.            -0.\n",
      "    -0.             0.             0.             0.\n",
      "     0.             0.            -0.             0.\n",
      "    20.14419415     0.             0.             0.\n",
      "     0.             0.           339.04468804     0.\n",
      "     0.           459.53799903    -0.             0.\n",
      "   122.69221826    -0.            91.41202242     0.\n",
      "    -0.             0.             0.            73.14365856\n",
      "     0.            -0.             0.             0.\n",
      "    86.35600042     0.             0.             0.\n",
      "  -104.57143405   264.93206555     0.            23.4488645\n",
      "   -49.39355188     0.             5.19775369     0.        ]\n",
      "Non-zero features: 20\n",
      "R-squared score (training): 0.631\n",
      "R-squared score (test): 0.624\n",
      "\n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\tPctKidsBornNeverMar, 1488.365\n",
      "\tPctKids2Par, -1188.740\n",
      "\tHousVacant, 459.538\n",
      "\tPctPersDenseHous, 339.045\n",
      "\tNumInShelters, 264.932\n",
      "\tMalePctDivorce, 259.329\n",
      "\tPctWorkMom, -231.423\n",
      "\tpctWInvInc, -169.676\n",
      "\tagePct12t29, -168.183\n",
      "\tPctVacantBoarded, 122.692\n",
      "\tpctUrban, 119.694\n",
      "\tMedOwnCostPctIncNoMtg, -104.571\n",
      "\tMedYrHousBuilt, 91.412\n",
      "\tRentQrange, 86.356\n",
      "\tOwnOccHiQuart, 73.144\n",
      "\tPctEmplManu, -57.530\n",
      "\tPctBornSameState, -49.394\n",
      "\tPctForeignBorn, 23.449\n",
      "\tPctLargHouseFam, 20.144\n",
      "\tPctSameCity85, 5.198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el modelo de regresión de Lasso nos ayuda a ver cuales son las variables que más influencia tienen entre los datos de entrada y salida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Que ocurre si variamos  𝛼  en el modelo Lasso?\n",
    "Usando un valor de $\\alpha=3$ obtenemos el mejor indicador $R^2$ en el test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.50\n",
      "Features kept: 35, r-squared training: 0.65, r-squared test: 0.58\n",
      "\n",
      "Alpha = 1.00\n",
      "Features kept: 25, r-squared training: 0.64, r-squared test: 0.60\n",
      "\n",
      "Alpha = 2.00\n",
      "Features kept: 20, r-squared training: 0.63, r-squared test: 0.62\n",
      "\n",
      "Alpha = 3.00\n",
      "Features kept: 17, r-squared training: 0.62, r-squared test: 0.63\n",
      "\n",
      "Alpha = 5.00\n",
      "Features kept: 12, r-squared training: 0.60, r-squared test: 0.61\n",
      "\n",
      "Alpha = 10.00\n",
      "Features kept: 6, r-squared training: 0.57, r-squared test: 0.58\n",
      "\n",
      "Alpha = 20.00\n",
      "Features kept: 2, r-squared training: 0.51, r-squared test: 0.50\n",
      "\n",
      "Alpha = 50.00\n",
      "Features kept: 1, r-squared training: 0.31, r-squared test: 0.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polinomial Regression\n",
    "Supongamos por un momento que tenemos un conjunto de datos 2 dimensionales ($x_0, x_1$). Podríamos agregas mas información que involucre la multiplicación de todas las combinaciones de caracteristicas entre si terminando con un dataset de 5 dimensiones: ($x_0$, $x_1$, $x_0^2$, $x_0x_1$, $x_1^2$).\n",
    "\n",
    "$ x = (x_0,x_1) \\quad \\rightarrow \\quad x'=(x_0,x_1,x_0^2,x_0x_1,x_1^2)$\n",
    "\n",
    "Ahora podríamos considerar el problema de predecir la misma regresión lineal pero esta vez usando 5 características en vez de las 2 originales. El punto importante aquí es que este aun es un problema de regresión lineal por lo que usamos la misma técnica usada antes\n",
    "\n",
    "$\\hat{y} = \\hat{w}_0x_0 + \\hat{w}_1x_1 + \\hat{w}_{00}x_0^2 + \\hat{w}_{01}x_0x_1 + \\hat{w}_{11}x_1^2 = \\sum_i\\hat{w}_ix_i + \\sum_{i,j} \\hat{w}_{i,j}x_ix_j$\n",
    "\n",
    "Este tipo problema se llama regresión polinomial, donde el número inicial de dimensiones del dataset da el orden del polinomio con el cual vamos a trabajar. En este caso es un polinomio de orden 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caracteristicas de una regresion lineal polinomial\n",
    "- Nos permite capturar interacciones entre las caracteristicas originales agragandolas como nuevas interacciones al modelo lineal\n",
    "- Para hacer un problema de clasificacion más fácil.\n",
    "- Podemos aplicar transformaciones no lienales para crear nuevas caracteristicas a analizar.\n",
    "\n",
    "Un efecto adverso de aplicar este tipo de regresiones es que genera modelos más complejos que pueden ser sobreajustados}, por lo tanto para aminorar este efecto es que generalmente van a compañados con una ragularización al igual que los modelos de regresión Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.73562365  9.63063021  8.54132022 21.21811721 10.91627127  6.09081028\n",
      " 18.41621638  9.65292317 18.25188238 12.14032442 12.07187785  6.68579564\n",
      " 22.08810712 14.88671165 12.57816757 15.10918004  8.22218446 20.24695243\n",
      " 11.55748746 17.41823211 15.87877067 22.78057004  9.18985112 18.50675171\n",
      " 22.39511633 16.33444873  4.33057892 17.2134918  11.30742883 11.61715968\n",
      "  8.59450992  9.27443963 12.36412887 25.28585219 19.0688687  24.20228984\n",
      " 19.35369672 13.05039828 11.72234556 16.73585851 17.76490158 12.87001621\n",
      " 15.22742039  7.13070042 20.21185351 14.26511075 12.94711274 11.42031629\n",
      "  9.55877354 19.44357164 10.23527752 21.63246599 19.1405913  12.61974534\n",
      "  9.46153769 13.274062   10.19963427 20.46488539  9.63399887 23.88924514\n",
      " 20.4618788   3.03696073  8.38020792 21.36781617 14.86941989 15.93572235\n",
      " 16.59336007 19.62413769 19.71092748  9.89245745 13.59296032 18.25888161\n",
      " 15.7675116   7.86400356  3.59181766]\n",
      "linear model coeff (w): [ 4.42036739  5.99661447  0.52894712 10.23751345  6.5507973  -2.02082636\n",
      " -0.32378811]\n",
      "linear model intercept (b): 1.543\n",
      "R-squared score (training): 0.722\n",
      "R-squared score (test): 0.722\n",
      "\n",
      "Now we transform the original input data to add\n",
      "polynomial features up to degree 2 (quadratic)\n",
      "\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 3.40951018e-12  1.66452443e+01  2.67285381e+01 -2.21348316e+01\n",
      "  1.24359227e+01  6.93086826e+00  1.04772675e+00  3.71352773e+00\n",
      " -1.33785505e+01 -5.73177185e+00  1.61813184e+00  3.66399592e+00\n",
      "  5.04513181e+00 -1.45835979e+00  1.95156872e+00 -1.51297378e+01\n",
      "  4.86762224e+00 -2.97084269e+00 -7.78370522e+00  5.14696078e+00\n",
      " -4.65479361e+00  1.84147395e+01 -2.22040650e+00  2.16572630e+00\n",
      " -1.27989481e+00  1.87946559e+00  1.52962716e-01  5.62073813e-01\n",
      " -8.91697516e-01 -2.18481128e+00  1.37595426e+00 -4.90336041e+00\n",
      " -2.23535458e+00  1.38268439e+00 -5.51908208e-01 -1.08795007e+00]\n",
      "(poly deg 2) linear model intercept (b): -3.206\n",
      "(poly deg 2) R-squared score (training): 0.969\n",
      "(poly deg 2) R-squared score (test): 0.805\n",
      "\n",
      "\n",
      "Addition of many polynomial features often leads to\n",
      "overfitting, so we often use polynomial features in combination\n",
      "with regression that has a regularization penalty, like ridge\n",
      "regression.\n",
      "\n",
      "(poly deg 2 + ridge) linear model coeff (w):\n",
      "[ 0.          2.229281    4.73349734 -3.15432089  3.8585194   1.60970912\n",
      " -0.76967054 -0.14956002 -1.75215371  1.5970487   1.37080607  2.51598244\n",
      "  2.71746523  0.48531538 -1.9356048  -1.62914955  1.51474518  0.88674141\n",
      "  0.26141199  2.04931775 -1.93025705  3.61850966 -0.71788143  0.63173956\n",
      " -3.16429847  1.29161448  3.545085    1.73422041  0.94347654 -0.51207219\n",
      "  1.70114448 -1.97949067  1.80687548 -0.2173863   2.87585898 -0.89423157]\n",
      "(poly deg 2 + ridge) linear model intercept (b): 5.418\n",
      "(poly deg 2 + ridge) R-squared score (training): 0.826\n",
      "(poly deg 2 + ridge) R-squared score (test): 0.825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import make_friedman1\n",
    "\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100, n_features = 7, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression.\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
