{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Lineales\n",
    "Un modelo lineal es una suma de variables pesadas que predice el valor de salida dado un valor de entrada.\n",
    "\n",
    "Ej: Predecir el valor del precio de casas.\n",
    "Dadas algunas caracter칤sticas de las casas como: impuestos por a침o ($X_{tax}$) y edad de la casa en a침os ($X_{age}$) podemos utilizar esa informaci칩n para estimar el precio de cada casa. Como ejemplo podemos escribir el siguiente modelo.\n",
    "\n",
    "$Y_{price} = C_{value} + w_{tax}X_{tax} + w_{age}X_{tax}$\n",
    "\n",
    "La idea principal es que usando ML podemos ajustar los valores de los pesos $w_i$ y el valor constante para poder predecir valores de casas basados en inputs anteriores.\n",
    "\n",
    "Este es un claro ejemplo de un modelo de regresion lineal.\n",
    "\n",
    "### Estructura de un modelo de regresion lineal\n",
    "De forma general este tipo de modelos tiene como input un conjunto de cracter칤sticas denominadas $x_i$, $i=0,\\dots,n$. Y cada caracter칤stica se le asocia un peso $w_i$, $i=0,\\dots,n$, de modo que el valor de salida sar치 la suma de los pesos por cada caracteristica mas un valor constante $b$:\n",
    "\n",
    "$\\hat{y} = \\sum_i \\hat{w}_ix_i + \\hat{b}$\n",
    "\n",
    "Los pesos $\\hat{w}_i$ y el intercepto $\\hat{b}$ son los parametros a estimar basados en el entrenamiendo del modelo. Una forma de encontrar estos parametros es a trav칠s del m칠todo de los m칤nimos cuadrados.\n",
    "Este m칠todo consiste en encontrar el valor que minimice el error medio cuadrado del modelo, es decir, el valor que minimice la suma de las diferencias al cuadrado entre el valor predicho de cada punto $\\hat{y}(x_i)$ y el valor real de cada punto $y(x)$, dividido por el n칰mero de puntos utilizados en el entrenamiento.\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum_{i=0}^n (\\hat{y}(x_i) - y(x))^2$\n",
    "\n",
    "Los modelos lineales hacen fuertes suposiciones sobre la relaci칩n de la variables de entrada y de salida. Pareciera que fueran simplistas, pero en datasets con muchas variables pueden ser muy efectivos y generalizar bien la din치mica de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 쮺omo son estimados los par치metros del modelo?\n",
    "Existen varias formas de estimar los par치metros del modelo a partir de los datos de entrenamiento. Esto va a depender de los criterios de fiteo, objetivos y formas de controlar la complejidad del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como implementar la Regresion Lineal\n",
    "Al igual que en los casos anteriores usamos la clase LinearRegression con los datos ya separados que usaremos para entrenar el modelo.\n",
    "Una vez entrenado, podemos acceder a los valores de $w_i$ y $b$ con el atributo coef_ y intercept_ respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear model coeff (w): [45.70870465]\n",
      "linear model intercept (b): 148.446\n",
      "R-squared score (training): 0.679\n",
      "R-squared score (test): 0.492\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias = 150.0,\n",
    "                            noise = 30, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
    "m = linreg.coef_\n",
    "b = linreg.intercept_\n",
    "y = m*X_R1 + b\n",
    "plt.plot(X_R1, y, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaci칩n del K-NN Regression vs Least-Squares Linear Regresion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante comparar los resultados de una regresi칩n lineal aplicando dos modelos distintos de ML al mismo conjunto de datos:\n",
    "- k-NN Regression\n",
    "- Least-Squares Lineal Regression\n",
    "\n",
    "En primer lugar el modelo k-NN no hace muchas suposiciones sobre la estructurta de los datos, por lo que su predicci칩n es potencialmente  precisa pero algunas veces genera predicciones inestable que son muy sensibles a peque침os cambios en los datos de entrenamiento. Esto hace que presente coeficiente de determinaci칩n mayor (para el training set) que el caso de regresi칩n lineal.\n",
    "\n",
    "Por el contrario, modelos lineales asumen muchas suposiciones sobre la estructura de los datos, dando como resultados predicciones mas estables pero a la vez m치s imprecisas, mostrando un coeficiente de determinaci칩n menor (para el training set) que el caso k-NN.\n",
    "\n",
    "<img src=\"k-NN_vs_LS_linear.png\">\n",
    "\n",
    "Para este caso particular, es importante resaltar que el modelo de regresion lineal obtuvo un $R^2$ ligeramente m치s alto que el caso k-NN para los datos de test, debido a que exist칤a una clara relaci칩n entre las variables haciendo que estos se ajustaran de mejor forma a la predicci칩n. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras formas de estimar los par치matros del modelo lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "Usa el mismo criterio de m칤nimos cuadrados que el caso anterior, con una peque침a diferencia, agrega una penalizaci칩n a los parametros $w_i$ que son muy grandes (a esto se le llama regularizaci칩n). La reguralizaci칩n es importante en ML dado que previene un sobreajuste (overfit) del modelo, reduciendo la complejidad de este.  \n",
    "\n",
    "$RSS_{RIDGE}(w,b) = \\sum_{i=1}^N (y(x_i) - (w\\cdot x_i + b))^2 + \\alpha \\sum_{j=1}^p w_j^2$\n",
    "\n",
    "La incorporaci칩n de la 칰ltima parte en la ecuaci칩n mostrada hace que modelos con pesos m치s grandes contribuyan m치s en la ecuaci칩n final. De esta forma, debido a que el objetivo final es minimizar la funcion final, la regularizaci칩n actua como una penalizaci칩n ($L_2$) a los pesos que mas contribuyen. As칤, en el caso en que tengamos 2 posibles predicciones, se preferir치 un modelo donde la suma de los pesos total sea menor. Este efecto no se aprecia en casos con 1 sola caracter칤stica, sino que en datasets con m칰ltiples caracteristicas, donde la regularizaci칩n puede incrementar de forma importante la eficiencia del modelo.\n",
    "\n",
    "La cantidad de regularizaci칩n que aplicamos est치 controlada por el par치metro $\\alpha$, indicando que valores m치s altos de $\\alpha$ implican mayor regularizaci칩n y por lo tanto menor complejidad en los modelo con pesos m치s cercanos a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementaci칩n de modelo de Regresi칩n Ridge\n",
    "Primero cargamos el dataframe con los datos de crimenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adspy_shared_utilities import load_crime_dataset\n",
    "crime = pd.read_table('/home/felipe/Documents/Programas/git_repository/Applied_Machine_Learning_in_python/2nd_week/CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "# remove features with poor coverage or lower relevance, and keep ViolentCrimesPerPop target column\n",
    "columns_to_keep = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]  \n",
    "crime = crime.ix[:,columns_to_keep].dropna()\n",
    "X_crime = crime.ix[:,range(0,88)]\n",
    "y_crime = crime['ViolentCrimesPerPop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la clase Ridge y pasando la informaci칩n del parametro $\\alpha$ podemos inmediatamente entrenar el modelo con la cunci칩n fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "ridge intercept: -3352.42, linear intercept: 3861.71\n",
      "R-squared score (training): ridge: 0.671,  linear: 0.668\n",
      "R-squared score (test): ridge: 0.494,  linear: 0.520\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge intercept: {:.2f}, linear intercept: {:.2f}'\n",
    "     .format(linridge.intercept_,linreg.intercept_))\n",
    "print('R-squared score (training): ridge: {:.3f},  linear: {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train),linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): ridge: {:.3f},  linear: {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test),linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos el resultado de Ridge con el modelo de regresion lineal simple podemos ver que no hay mayor diferencia en los valores de $R^2$. Sin embargo, podemos realizar algunos cambios a modelo Ridge para mejorar los resultados. Lo que podemos hacer es normalizar las escalas de los pesos de cada caracter칤stica que originalmente presentaban distintas escalas. Existen varias formas de normalizar los pesos, pero en esta oportunidad utilizaremos un tipo llamado reescalamiento MinMax, haciendo que todas las caracter칤sticas est칠n evaluadas entre 0 y 1:\n",
    "- A cada caracteristica $x_i$ calcularemos el valor m칤nimo y m치ximo y transformaremos de $x_i$ a $x'_i$ de la siguiente forma:\n",
    "\n",
    "$x'_i = \\frac{x_i - x_i^{min}}{x_i^{max} - x_i^{min}}$\n",
    "\n",
    "Para esto debemos:\n",
    "- importar MinMaxScaler \n",
    "- preparar el objeto escalar para su uso usando scaler.fit(X_train)\n",
    "- aplicamos el reescalamiento con scaler.transform()\n",
    "- Podemos ser un poco mas eficientes aplicando al mismo el fit y la transformaci칩n al training set con el comando scaler.fit_transform(X_train)\n",
    "\n",
    "Algunos consejos a la hora de realizar la normalizacon:\n",
    "- fitear el scaler usando el trainig set y luego aplicar el mnismo scaler al train set.\n",
    "- No escalar el training y test set usando diferentes scalers: esto causar칤a un sesgo en los datos.\n",
    "- No fit el scaler con el test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "ridge scaled intercept: 933.39, linear intercept: 3861.71\n",
      "R-squared score (training): ridge scaled: 0.615,  linear: 0.668\n",
      "R-squared score (test): ridge scaled: 0.599,  linear: 0.520\n",
      "Improvement of 21.16% in the test set prediction\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "#Aplicamos el reescalamiento a X_train y X_test\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge_scaled = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge scaled intercept: {:.2f}, linear intercept: {:.2f}'\n",
    "     .format(linridge_scaled.intercept_,linreg.intercept_))\n",
    "print('R-squared score (training): ridge scaled: {:.3f},  linear: {:.3f}'\n",
    "     .format(linridge_scaled.score(X_train_scaled, y_train),linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): ridge scaled: {:.3f},  linear: {:.3f}'\n",
    "     .format(linridge_scaled.score(X_test_scaled, y_test),linreg.score(X_test, y_test)))\n",
    "eff = 100*(linridge_scaled.score(X_test_scaled, y_test)- linridge.score(X_test, y_test))/linridge.score(X_test, y_test)\n",
    "print('Improvement of {:.2f}% in the test set prediction'.format(eff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 쯈ue ocurre si variamos $\\alpha$ en el modelo Ridge?\n",
    "El mejor $R^2$ en el test set resulta ser cuando usamos $\\alpha=20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression: effect of alpha regularization parameter\n",
      "\n",
      "Alpha = 0.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.67, r-squared test: 0.50\n",
      "\n",
      "Alpha = 1.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.66, r-squared test: 0.56\n",
      "\n",
      "Alpha = 10.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.63, r-squared test: 0.59\n",
      "\n",
      "Alpha = 20.00\n",
      "num abs(coeff) > 1.0: 88, r-squared training: 0.61, r-squared test: 0.60\n",
      "\n",
      "Alpha = 50.00\n",
      "num abs(coeff) > 1.0: 86, r-squared training: 0.58, r-squared test: 0.58\n",
      "\n",
      "Alpha = 100.00\n",
      "num abs(coeff) > 1.0: 87, r-squared training: 0.55, r-squared test: 0.55\n",
      "\n",
      "Alpha = 1000.00\n",
      "num abs(coeff) > 1.0: 84, r-squared training: 0.31, r-squared test: 0.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Al igual que el metodo de Ridge, Lasso tambi칠n agrega un factor de penalizaci칩n ($L_1$) a los pesos, pero esta vez usa el valor absoluto del peso en vez del peso al cuadrado\n",
    "\n",
    "$RSS_{LASSO}(w,b) = \\sum_{i=1}^N (y(x_i) - (w\\cdot x_i + b))^2 + \\alpha \\sum_{j=1}^p |w_j|$\n",
    "\n",
    "El efecto de esta forma de regularizaci칩n es hacer que los pesos de los parametros que menos contribuyen sean 0, dejando con un valor distinto de 0 a los pesos m치s influyentes. Esta forma de regularizaci칩n tambi칠n esta controlada por el par치metro $\\alpha$.\n",
    "\n",
    "Basados en la forma en como se regularizan las regresiones lineales, ya sea usando Ridge ($L_2$) o Lasso ($L_1$), podemos distinguir en que casos usar una o la otra forma:\n",
    "- Cuando tenemos muchas caracter칤sticas ($x_i$) con efectos peque침os o medianos usamos Ridge\n",
    "- Cuando tenemos pocas caractet칤sticas ($x_i$) con efectos medianos o largos usamos Lasso\n",
    "\n",
    "Para implementar la regresi칩n Lasso debemos importar la clase Lasso.\n",
    "En algunos casos, aparecer치 en pantalla una advertencia de convergencia. Para evitar eso debemos incorporar la variable max_iter con un valor grande, frecuentemente se usa por lo menos 20000 o m치s. Esto aumentar치 el tiempo de iteraci칩n.\n",
    "Usando el dataset de los crimenes podemos ver que solo algunos pesos han quedado con valores distintos de 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime dataset\n",
      "lasso regression linear model intercept: 1186.6120619985793\n",
      "lasso regression linear model coeff:\n",
      "[    0.             0.            -0.          -168.18346054\n",
      "    -0.            -0.             0.           119.6938194\n",
      "     0.            -0.             0.          -169.67564456\n",
      "    -0.             0.            -0.             0.\n",
      "     0.             0.            -0.            -0.\n",
      "     0.            -0.             0.             0.\n",
      "   -57.52991966    -0.            -0.             0.\n",
      "   259.32889226    -0.             0.             0.\n",
      "     0.            -0.         -1188.7396867     -0.\n",
      "    -0.            -0.          -231.42347299     0.\n",
      "  1488.36512229     0.            -0.            -0.\n",
      "    -0.             0.             0.             0.\n",
      "     0.             0.            -0.             0.\n",
      "    20.14419415     0.             0.             0.\n",
      "     0.             0.           339.04468804     0.\n",
      "     0.           459.53799903    -0.             0.\n",
      "   122.69221826    -0.            91.41202242     0.\n",
      "    -0.             0.             0.            73.14365856\n",
      "     0.            -0.             0.             0.\n",
      "    86.35600042     0.             0.             0.\n",
      "  -104.57143405   264.93206555     0.            23.4488645\n",
      "   -49.39355188     0.             5.19775369     0.        ]\n",
      "Non-zero features: 20\n",
      "R-squared score (training): 0.631\n",
      "R-squared score (test): 0.624\n",
      "\n",
      "Features with non-zero weight (sorted by absolute magnitude):\n",
      "\tPctKidsBornNeverMar, 1488.365\n",
      "\tPctKids2Par, -1188.740\n",
      "\tHousVacant, 459.538\n",
      "\tPctPersDenseHous, 339.045\n",
      "\tNumInShelters, 264.932\n",
      "\tMalePctDivorce, 259.329\n",
      "\tPctWorkMom, -231.423\n",
      "\tpctWInvInc, -169.676\n",
      "\tagePct12t29, -168.183\n",
      "\tPctVacantBoarded, 122.692\n",
      "\tpctUrban, 119.694\n",
      "\tMedOwnCostPctIncNoMtg, -104.571\n",
      "\tMedYrHousBuilt, 91.412\n",
      "\tRentQrange, 86.356\n",
      "\tOwnOccHiQuart, 73.144\n",
      "\tPctEmplManu, -57.530\n",
      "\tPctBornSameState, -49.394\n",
      "\tPctForeignBorn, 23.449\n",
      "\tPctLargHouseFam, 20.144\n",
      "\tPctSameCity85, 5.198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el modelo de regresi칩n de Lasso nos ayuda a ver cuales son las variables que m치s influencia tienen entre los datos de entrada y salida. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 쯈ue ocurre si variamos  洧띺  en el modelo Lasso?\n",
    "Usando un valor de $\\alpha=3$ obtenemos el mejor indicador $R^2$ en el test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso regression: effect of alpha regularization\n",
      "parameter on number of features kept in final model\n",
      "\n",
      "Alpha = 0.50\n",
      "Features kept: 35, r-squared training: 0.65, r-squared test: 0.58\n",
      "\n",
      "Alpha = 1.00\n",
      "Features kept: 25, r-squared training: 0.64, r-squared test: 0.60\n",
      "\n",
      "Alpha = 2.00\n",
      "Features kept: 20, r-squared training: 0.63, r-squared test: 0.62\n",
      "\n",
      "Alpha = 3.00\n",
      "Features kept: 17, r-squared training: 0.62, r-squared test: 0.63\n",
      "\n",
      "Alpha = 5.00\n",
      "Features kept: 12, r-squared training: 0.60, r-squared test: 0.61\n",
      "\n",
      "Alpha = 10.00\n",
      "Features kept: 6, r-squared training: 0.57, r-squared test: 0.58\n",
      "\n",
      "Alpha = 20.00\n",
      "Features kept: 2, r-squared training: 0.51, r-squared test: 0.50\n",
      "\n",
      "Alpha = 50.00\n",
      "Features kept: 1, r-squared training: 0.31, r-squared test: 0.30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polinomial Regression\n",
    "Supongamos por un momento que tenemos un conjunto de datos 2 dimensionales ($x_0, x_1$). Podr칤amos agregas mas informaci칩n que involucre la multiplicaci칩n de todas las combinaciones de caracteristicas entre si terminando con un dataset de 5 dimensiones: ($x_0$, $x_1$, $x_0^2$, $x_0x_1$, $x_1^2$).\n",
    "\n",
    "$ x = (x_0,x_1) \\quad \\rightarrow \\quad x'=(x_0,x_1,x_0^2,x_0x_1,x_1^2)$\n",
    "\n",
    "Ahora podr칤amos considerar el problema de predecir la misma regresi칩n lineal pero esta vez usando 5 caracter칤sticas en vez de las 2 originales. El punto importante aqu칤 es que este aun es un problema de regresi칩n lineal por lo que usamos la misma t칠cnica usada antes\n",
    "\n",
    "$\\hat{y} = \\hat{w}_0x_0 + \\hat{w}_1x_1 + \\hat{w}_{00}x_0^2 + \\hat{w}_{01}x_0x_1 + \\hat{w}_{11}x_1^2 = \\sum_i\\hat{w}_ix_i + \\sum_{i,j} \\hat{w}_{i,j}x_ix_j$\n",
    "\n",
    "Este tipo problema se llama regresi칩n polinomial, donde el n칰mero inicial de dimensiones del dataset da el orden del polinomio con el cual vamos a trabajar. En este caso es un polinomio de orden 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caracteristicas de una regresion lineal polinomial\n",
    "- Nos permite capturar interacciones entre las caracteristicas originales agragandolas como nuevas interacciones al modelo lineal\n",
    "- Para hacer un problema de clasificacion m치s f치cil.\n",
    "- Podemos aplicar transformaciones no lienales para crear nuevas caracteristicas a analizar.\n",
    "\n",
    "Un efecto adverso de aplicar este tipo de regresiones es que genera modelos m치s complejos que pueden ser sobreajustados}, por lo tanto para aminorar este efecto es que generalmente van a compa침ados con una ragularizaci칩n al igual que los modelos de regresi칩n Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.73562365  9.63063021  8.54132022 21.21811721 10.91627127  6.09081028\n",
      " 18.41621638  9.65292317 18.25188238 12.14032442 12.07187785  6.68579564\n",
      " 22.08810712 14.88671165 12.57816757 15.10918004  8.22218446 20.24695243\n",
      " 11.55748746 17.41823211 15.87877067 22.78057004  9.18985112 18.50675171\n",
      " 22.39511633 16.33444873  4.33057892 17.2134918  11.30742883 11.61715968\n",
      "  8.59450992  9.27443963 12.36412887 25.28585219 19.0688687  24.20228984\n",
      " 19.35369672 13.05039828 11.72234556 16.73585851 17.76490158 12.87001621\n",
      " 15.22742039  7.13070042 20.21185351 14.26511075 12.94711274 11.42031629\n",
      "  9.55877354 19.44357164 10.23527752 21.63246599 19.1405913  12.61974534\n",
      "  9.46153769 13.274062   10.19963427 20.46488539  9.63399887 23.88924514\n",
      " 20.4618788   3.03696073  8.38020792 21.36781617 14.86941989 15.93572235\n",
      " 16.59336007 19.62413769 19.71092748  9.89245745 13.59296032 18.25888161\n",
      " 15.7675116   7.86400356  3.59181766]\n",
      "linear model coeff (w): [ 4.42036739  5.99661447  0.52894712 10.23751345  6.5507973  -2.02082636\n",
      " -0.32378811]\n",
      "linear model intercept (b): 1.543\n",
      "R-squared score (training): 0.722\n",
      "R-squared score (test): 0.722\n",
      "\n",
      "Now we transform the original input data to add\n",
      "polynomial features up to degree 2 (quadratic)\n",
      "\n",
      "(poly deg 2) linear model coeff (w):\n",
      "[ 3.40951018e-12  1.66452443e+01  2.67285381e+01 -2.21348316e+01\n",
      "  1.24359227e+01  6.93086826e+00  1.04772675e+00  3.71352773e+00\n",
      " -1.33785505e+01 -5.73177185e+00  1.61813184e+00  3.66399592e+00\n",
      "  5.04513181e+00 -1.45835979e+00  1.95156872e+00 -1.51297378e+01\n",
      "  4.86762224e+00 -2.97084269e+00 -7.78370522e+00  5.14696078e+00\n",
      " -4.65479361e+00  1.84147395e+01 -2.22040650e+00  2.16572630e+00\n",
      " -1.27989481e+00  1.87946559e+00  1.52962716e-01  5.62073813e-01\n",
      " -8.91697516e-01 -2.18481128e+00  1.37595426e+00 -4.90336041e+00\n",
      " -2.23535458e+00  1.38268439e+00 -5.51908208e-01 -1.08795007e+00]\n",
      "(poly deg 2) linear model intercept (b): -3.206\n",
      "(poly deg 2) R-squared score (training): 0.969\n",
      "(poly deg 2) R-squared score (test): 0.805\n",
      "\n",
      "\n",
      "Addition of many polynomial features often leads to\n",
      "overfitting, so we often use polynomial features in combination\n",
      "with regression that has a regularization penalty, like ridge\n",
      "regression.\n",
      "\n",
      "(poly deg 2 + ridge) linear model coeff (w):\n",
      "[ 0.          2.229281    4.73349734 -3.15432089  3.8585194   1.60970912\n",
      " -0.76967054 -0.14956002 -1.75215371  1.5970487   1.37080607  2.51598244\n",
      "  2.71746523  0.48531538 -1.9356048  -1.62914955  1.51474518  0.88674141\n",
      "  0.26141199  2.04931775 -1.93025705  3.61850966 -0.71788143  0.63173956\n",
      " -3.16429847  1.29161448  3.545085    1.73422041  0.94347654 -0.51207219\n",
      "  1.70114448 -1.97949067  1.80687548 -0.2173863   2.87585898 -0.89423157]\n",
      "(poly deg 2 + ridge) linear model intercept (b): 5.418\n",
      "(poly deg 2 + ridge) R-squared score (training): 0.826\n",
      "(poly deg 2 + ridge) R-squared score (test): 0.825\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.datasets import make_friedman1\n",
    "\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100, n_features = 7, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression.\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
